{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization,MaxPool2D\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.losses import CategoricalCrossentropy,SparseCategoricalCrossentropy\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Damage Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "1. For Handling Imbalanced Dataset\n",
    "    * performed flips on existing class images with less quantity\n",
    "    * Added new labels and images to existing dataset\n",
    "    * performed stratified Split for train , \n",
    "    * created categorical encoding for multilabel classfication\n",
    "    \n",
    "2. For small dataset\n",
    "    * performed augmentation\n",
    "    * performed further image preprocesssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = r\"C:\\projects\\contelligenz\\damage_detect\\Train Copy\\790.jpeg\"# change\n",
    "# img = cv2.imread(filename)\n",
    "# Flip_Horizontal = cv2.flip(img, 1) # 1 means Horizontal Flip\n",
    "# cv2.imwrite(f\"./templamp/1097.jpeg\", Flip_Horizontal) # change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('./newtrainlabels.csv')\n",
    "labels['class'] = labels['class'].apply(lambda x: [i.strip() for i in x.split(\",\")])\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "x = labels.drop(['class'],axis=1).values\n",
    "y = labels['class']\n",
    "mlb = MultiLabelBinarizer()\n",
    "actuals = mlb.fit_transform(y)\n",
    "x_train,y_train,x_test,y_test = iterative_train_test_split(x, actuals, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dent', 'glass_shatter', 'head_lamp', 'scratch', 'tail_lamp',\n",
       "       'unknown'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dent 226\n",
      "glass_shatter 189\n",
      "head_lamp 198\n",
      "scratch 213\n",
      "tail_lamp 167\n",
      "unknown 148\n"
     ]
    }
   ],
   "source": [
    "print(\"dent\",np.count_nonzero(y_train[:,0])) # dent\n",
    "print(\"glass_shatter\",np.count_nonzero(y_train[:,1])) # dent\n",
    "print(\"head_lamp\",np.count_nonzero(y_train[:,2])) # dent\n",
    "print(\"scratch\",np.count_nonzero(y_train[:,3])) # dent\n",
    "print(\"tail_lamp\",np.count_nonzero(y_train[:,4])) # dent\n",
    "print(\"unknown\",np.count_nonzero(y_train[:,5])) # dent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "image_size= (224, 224)\n",
    "learning_rate=0.0001\n",
    "path = \"./datasets\"\n",
    "num_epochs= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(None)\n",
    "train[['image','damage','subset']] = x_train\n",
    "train['y'] = pd.Series(y_train.tolist())\n",
    "train['image'] = train['image'].apply(lambda x: x.split('/')[-1])\n",
    "train['y_new'] = train['y'].apply(lambda x: [i for i in mlb.inverse_transform(np.array(x,ndmin=2))[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.DataFrame(None)\n",
    "valid[['image','damage','subset']] = x_test\n",
    "valid['y'] = pd.Series(y_test.tolist())\n",
    "valid['image'] = valid['image'].apply(lambda x: x.split('/')[-1])\n",
    "valid['y_new'] = valid['y'].apply(lambda x: [i for i in mlb.inverse_transform(np.array(x,ndmin=2))[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import preprocess_input\n",
    "def load_dataset(dir_path,train,valid):\n",
    "\n",
    "    img_gen1 = ImageDataGenerator(rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        vertical_flip=True,\n",
    "        ) \n",
    "\n",
    "    train_ds = img_gen1.flow_from_dataframe(\n",
    "        dataframe=train,\n",
    "        directory=dir_path,\n",
    "        x_col=\"image\",\n",
    "        y_col=\"y_new\",\n",
    "        batch_size=12,\n",
    "        seed=0,\n",
    "        shuffle=True,\n",
    "        class_mode=\"categorical\",\n",
    "        classes=[\"unknown\", \"dent\", \"scratch\", \"glass_shatter\", \"head_lamp\",\"tail_lamp\"],\n",
    "        target_size=image_size) # set as training data\n",
    "\n",
    "    img_gen2 = ImageDataGenerator(rescale=1./255,\n",
    "        ) \n",
    "    val_ds = img_gen2.flow_from_dataframe(\n",
    "        dataframe=valid,\n",
    "        directory=dir_path,\n",
    "        x_col=\"image\",\n",
    "        y_col=\"y_new\",\n",
    "        batch_size=12,\n",
    "        seed=0,\n",
    "        shuffle=True,\n",
    "        class_mode=\"categorical\",\n",
    "        classes=[\"unknown\", \"dent\", \"scratch\", \"glass_shatter\", \"head_lamp\",\"tail_lamp\"],\n",
    "        target_size=image_size,) # set as validation data\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 865 validated image filenames belonging to 6 classes.\n",
      "Found 216 validated image filenames belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\contelligenz\\venv\\lib\\site-packages\\keras\\preprocessing\\image.py:991: UserWarning: Found 2 invalid image filename(s) in x_col=\"image\". These filename(s) will be ignored.\n",
      "  n_invalid, x_col))\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds= load_dataset('./Train Copy',train.sample(train.shape[0]).reset_index(),valid.sample(valid.shape[0]).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unknown': 0,\n",
       " 'dent': 1,\n",
       " 'scratch': 2,\n",
       " 'glass_shatter': 3,\n",
       " 'head_lamp': 4,\n",
       " 'tail_lamp': 5}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unknown': 0,\n",
       " 'dent': 1,\n",
       " 'scratch': 2,\n",
       " 'glass_shatter': 3,\n",
       " 'head_lamp': 4,\n",
       " 'tail_lamp': 5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "1. For model training\n",
    "    * Created custom model with 14 layers.\n",
    "    * used Dropouts and Regularisation.\n",
    "    * used sigmoid activation for multilabel classfication\n",
    "\n",
    "2. For model compilation\n",
    "    * used adam optimizer\n",
    "    * used binary loss\n",
    "    * used fbeta2 for metrics for multilabel classification\n",
    "    * trained for 80 epochs\n",
    "\n",
    "3. For monitoring\n",
    "    * used checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "\n",
    "def model_detect(num_expressions=6):\n",
    "    model = VGG16(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224,224,3),\n",
    "    pooling='max',\n",
    "    classes=num_expressions,\n",
    ")\n",
    "    regularizer = tf.keras.regularizers.l2(0.1)\n",
    "    for layer in model.layers[:-5]:\n",
    "      layer.trainable= False\n",
    "    for layer in model.layers:\n",
    "      if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        print('Adding regularizer to layer {}'.format(layer.name))\n",
    "        layer.kernel_regularizer = regularizer\n",
    "\n",
    "    # model.get_layer('block5_conv1').trainable = True\n",
    "    # model.get_layer('block5_conv2').trainable = True\n",
    "    # model.get_layer('block5_conv3').trainable = True\n",
    "    # model.get_layer('block5_pool').trainable = True\n",
    "    flat1 = Flatten()(model.layers[-1].output)\n",
    "    layer1 = Dropout(0.4)(flat1)\n",
    "    layer2 = Dense(128, activation='relu', kernel_initializer='he_uniform')(layer1)\n",
    "    # layer3 = Dense(64, activation='relu', kernel_initializer='he_uniform')(layer2)\n",
    "    output = Dense(6, activation='sigmoid')(layer2)\n",
    "    # define new model\n",
    "    model = Model(inputs=model.inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "def define_model(in_shape=(224, 224, 3), out_shape=6):\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), activation='relu',  padding='same', input_shape=in_shape))\n",
    "  model.add(Conv2D(32, (3, 3), activation='relu',  padding='same'))\n",
    "  model.add(MaxPooling2D((2, 2)))\n",
    "  model.add(Conv2D(64, (3, 3), activation='relu',  padding='same'))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Conv2D(64, (3, 3), activation='relu',  padding='same',kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "  model.add(MaxPooling2D((2, 2)))\n",
    "  model.add(Conv2D(128, (3, 3), activation='relu',  padding='same'))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Conv2D(128, (3, 3), activation='relu',  padding='same',kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "  model.add(MaxPooling2D((2, 2)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "  model.add(Dense(out_shape, activation='sigmoid'))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend\n",
    " \n",
    "# calculate fbeta score for multi-class/label classification\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "\t# clip predictions\n",
    "\ty_pred = backend.clip(y_pred, 0, 1)\n",
    "\t# calculate elements\n",
    "\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "\t# calculate precision\n",
    "\tp = tp / (tp + fp + backend.epsilon())\n",
    "\t# calculate recall\n",
    "\tr = tp / (tp + fn + backend.epsilon())\n",
    "\t# calculate fbeta, averaged across each class\n",
    "\tbb = beta ** 2\n",
    "\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
    "\treturn fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding regularizer to layer block1_conv1\n",
      "Adding regularizer to layer block1_conv2\n",
      "Adding regularizer to layer block2_conv1\n",
      "Adding regularizer to layer block2_conv2\n",
      "Adding regularizer to layer block3_conv1\n",
      "Adding regularizer to layer block3_conv2\n",
      "Adding regularizer to layer block3_conv3\n",
      "Adding regularizer to layer block4_conv1\n",
      "Adding regularizer to layer block4_conv2\n",
      "Adding regularizer to layer block4_conv3\n",
      "Adding regularizer to layer block5_conv1\n",
      "Adding regularizer to layer block5_conv2\n",
      "Adding regularizer to layer block5_conv3\n",
      "Epoch 1/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.5540 - fbeta: 0.1026\n",
      "Epoch 1: val_fbeta improved from -inf to 0.08131, saving model to ./improvements1_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 54s 261ms/step - loss: 0.5540 - fbeta: 0.1026 - val_loss: 0.4501 - val_fbeta: 0.0813\n",
      "Epoch 2/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.4314 - fbeta: 0.2786\n",
      "Epoch 2: val_fbeta improved from 0.08131 to 0.31036, saving model to ./improvements1_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 14s 199ms/step - loss: 0.4314 - fbeta: 0.2786 - val_loss: 0.3877 - val_fbeta: 0.3104\n",
      "Epoch 3/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.3676 - fbeta: 0.4714\n",
      "Epoch 3: val_fbeta improved from 0.31036 to 0.50400, saving model to ./improvements1_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.3676 - fbeta: 0.4714 - val_loss: 0.3710 - val_fbeta: 0.5040\n",
      "Epoch 4/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.3300 - fbeta: 0.5352\n",
      "Epoch 4: val_fbeta improved from 0.50400 to 0.61312, saving model to ./improvements1_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 14s 196ms/step - loss: 0.3300 - fbeta: 0.5352 - val_loss: 0.3102 - val_fbeta: 0.6131\n",
      "Epoch 5/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2910 - fbeta: 0.6422\n",
      "Epoch 5: val_fbeta improved from 0.61312 to 0.65949, saving model to ./improvements1_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 16s 217ms/step - loss: 0.2910 - fbeta: 0.6422 - val_loss: 0.3058 - val_fbeta: 0.6595\n",
      "Epoch 6/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2502 - fbeta: 0.7076\n",
      "Epoch 6: val_fbeta improved from 0.65949 to 0.67344, saving model to ./improvements1_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 14s 198ms/step - loss: 0.2502 - fbeta: 0.7076 - val_loss: 0.2941 - val_fbeta: 0.6734\n",
      "Epoch 7/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2274 - fbeta: 0.7480\n",
      "Epoch 7: val_fbeta improved from 0.67344 to 0.70517, saving model to ./improvements1_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 16s 222ms/step - loss: 0.2274 - fbeta: 0.7480 - val_loss: 0.2744 - val_fbeta: 0.7052\n",
      "Epoch 8/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1995 - fbeta: 0.7853\n",
      "Epoch 8: val_fbeta did not improve from 0.70517\n",
      "72/72 [==============================] - 14s 192ms/step - loss: 0.1995 - fbeta: 0.7853 - val_loss: 0.2844 - val_fbeta: 0.6945\n",
      "Epoch 9/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1674 - fbeta: 0.8436\n",
      "Epoch 9: val_fbeta improved from 0.70517 to 0.73906, saving model to ./improvements1_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 15s 214ms/step - loss: 0.1674 - fbeta: 0.8436 - val_loss: 0.2641 - val_fbeta: 0.7391\n",
      "Epoch 10/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1584 - fbeta: 0.8329\n",
      "Epoch 10: val_fbeta did not improve from 0.73906\n",
      "72/72 [==============================] - 14s 192ms/step - loss: 0.1584 - fbeta: 0.8329 - val_loss: 0.2664 - val_fbeta: 0.7148\n",
      "Epoch 11/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1479 - fbeta: 0.8597\n",
      "Epoch 11: val_fbeta did not improve from 0.73906\n",
      "72/72 [==============================] - 14s 193ms/step - loss: 0.1479 - fbeta: 0.8597 - val_loss: 0.3225 - val_fbeta: 0.7005\n",
      "Epoch 12/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1280 - fbeta: 0.8839\n",
      "Epoch 12: val_fbeta improved from 0.73906 to 0.75202, saving model to ./improvements1_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.1280 - fbeta: 0.8839 - val_loss: 0.2730 - val_fbeta: 0.7520\n",
      "Epoch 13/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1145 - fbeta: 0.9029\n",
      "Epoch 13: val_fbeta did not improve from 0.75202\n",
      "72/72 [==============================] - 14s 193ms/step - loss: 0.1145 - fbeta: 0.9029 - val_loss: 0.3072 - val_fbeta: 0.7366\n",
      "Epoch 14/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1017 - fbeta: 0.9083\n",
      "Epoch 14: val_fbeta did not improve from 0.75202\n",
      "72/72 [==============================] - 14s 193ms/step - loss: 0.1017 - fbeta: 0.9083 - val_loss: 0.2859 - val_fbeta: 0.7505\n",
      "Epoch 15/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0928 - fbeta: 0.9246\n",
      "Epoch 15: val_fbeta improved from 0.75202 to 0.77825, saving model to ./improvements1_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 15s 209ms/step - loss: 0.0928 - fbeta: 0.9246 - val_loss: 0.2622 - val_fbeta: 0.7783\n",
      "Epoch 16/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0831 - fbeta: 0.9326\n",
      "Epoch 16: val_fbeta did not improve from 0.77825\n",
      "72/72 [==============================] - 14s 193ms/step - loss: 0.0831 - fbeta: 0.9326 - val_loss: 0.2622 - val_fbeta: 0.7670\n",
      "Epoch 17/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0713 - fbeta: 0.9500\n",
      "Epoch 17: val_fbeta improved from 0.77825 to 0.78259, saving model to ./improvements1_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 15s 201ms/step - loss: 0.0713 - fbeta: 0.9500 - val_loss: 0.2493 - val_fbeta: 0.7826\n",
      "Epoch 18/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0783 - fbeta: 0.9248\n",
      "Epoch 18: val_fbeta improved from 0.78259 to 0.83558, saving model to ./improvements1_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 16s 222ms/step - loss: 0.0783 - fbeta: 0.9248 - val_loss: 0.2398 - val_fbeta: 0.8356\n",
      "Epoch 19/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0720 - fbeta: 0.9479\n",
      "Epoch 19: val_fbeta did not improve from 0.83558\n",
      "72/72 [==============================] - 14s 193ms/step - loss: 0.0720 - fbeta: 0.9479 - val_loss: 0.2649 - val_fbeta: 0.8033\n",
      "Epoch 20/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0731 - fbeta: 0.9463\n",
      "Epoch 20: val_fbeta did not improve from 0.83558\n",
      "72/72 [==============================] - 14s 193ms/step - loss: 0.0731 - fbeta: 0.9463 - val_loss: 0.3523 - val_fbeta: 0.7472\n",
      "Epoch 21/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0579 - fbeta: 0.9562\n",
      "Epoch 21: val_fbeta did not improve from 0.83558\n",
      "72/72 [==============================] - 14s 193ms/step - loss: 0.0579 - fbeta: 0.9562 - val_loss: 0.2586 - val_fbeta: 0.8102\n",
      "Epoch 22/80\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0573 - fbeta: 0.9555"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13956\\51046752.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     )\n",
      "\u001b[1;32mc:\\projects\\contelligenz\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\contelligenz\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1454\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1455\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1456\u001b[1;33m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[0;32m   1457\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1458\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\contelligenz\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\contelligenz\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1754\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1756\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1757\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1758\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\contelligenz\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\contelligenz\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\contelligenz\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mc:\\projects\\contelligenz\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2454\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2456\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\contelligenz\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1859\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1861\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\contelligenz\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    503\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\projects\\contelligenz\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exp_model = model_detect()\n",
    "# exp_model = keras.models.load_model('./improvements1/weights-improvement-best_model.hdf5')\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "# exp_model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "exp_model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
    "\n",
    "\n",
    "filepath=\"./improvements1_1/weights-improvement-best_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_fbeta', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = exp_model.fit(\n",
    "    \n",
    "    train_ds,\n",
    "    steps_per_epoch = train_ds.samples // 12,\n",
    "    validation_data = val_ds, \n",
    "    validation_steps = val_ds.samples // 12,\n",
    "    epochs = 80,\n",
    "    callbacks=callbacks_list\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.55415"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "1. Get the best model from checkpoint\n",
    "2. Get inverse coding for label outputs\n",
    "3. used 0.6 threshold for selecting labels\n",
    "4. monitored fbeta score for selecting threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import load_img, img_to_array\n",
    "\n",
    "def make_label_predicts(filename,model,thres=0.5):\n",
    "    # labs = {0:'dent', 1:'glass_shatter',2: 'head_lamp',3: 'scratch', 4:'tail_lamp',5: 'unknown'}\n",
    "    labs = {0:'unknown',\n",
    " 1:'dent',  \n",
    " 2:'scratch',\n",
    " 3:'glass_shatter',\n",
    " 4:'head_lamp',\n",
    " 5:'tail_lamp'}\n",
    "    img = cv2.imread(f\"{filename}\")\n",
    "    if img is not None:\n",
    "        # img = load_img(filename, target_size=(224, 224))\n",
    "        # x = img_to_array(img) /255.0\n",
    "        # x = np.expand_dims(x, axis=0)\n",
    "        # x = preprocess_input(x)\n",
    "        img = cv2.resize(img,(224, 224),interpolation=cv2.INTER_AREA)\n",
    "        img = np.array(img) / 225.0\n",
    "        img = img.reshape(-1,224,224,3)\n",
    "        pred = np.squeeze(model.predict(img,verbose=0))\n",
    "        # pred = np.squeeze(model.predict(x,verbose=0))\n",
    "        # print('Predicted:', model.predict(img,verbose=0)) #decode_predictions(pred)[0])\n",
    "        labels = []\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i]>=thres:\n",
    "                labels.append(labs[i])\n",
    "        if len(labels)==0:\n",
    "            labels.append('unknown')\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metric calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score: 0.8915681024146647\n",
      "accuracy_score:  0.83645443196005\n",
      "jaccard_score:  0.8060925046257145\n",
      "beta2: 0.8977177407352604\n",
      "[[[503  15]\n",
      "  [ 32 251]]\n",
      "\n",
      " [[657  12]\n",
      "  [ 12 120]]\n",
      "\n",
      " [[613  20]\n",
      "  [ 10 158]]\n",
      "\n",
      " [[502  40]\n",
      "  [ 14 245]]\n",
      "\n",
      " [[680  11]\n",
      "  [ 27  83]]\n",
      "\n",
      " [[590  26]\n",
      "  [ 13 172]]]\n"
     ]
    }
   ],
   "source": [
    "mod = keras.models.load_model(\"./improvements1_1/weights-improvement-best_model.hdf5\",custom_objects={'fbeta':fbeta})\n",
    "predictions = {}\n",
    "files = glob.glob('./Train/*')\n",
    "c = 0\n",
    "for f in files:\n",
    "    fname = f.split('\\\\')[-1]\n",
    "    predictions[f\"image/{fname}\"] = make_label_predicts(f\"./Train/{fname}\",mod,thres=0.6)\n",
    "\n",
    "sub1 = pd.DataFrame({'image':predictions.keys(),'class':predictions.values()})\n",
    "labeldf = pd.read_csv('./trainlabels.csv')\n",
    "hash_map  = dict(labeldf[['image','class']].values)\n",
    "actual = sub1['image'].apply(lambda x: [i.strip() for i in hash_map[x].split(',')])\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "actuals = mlb.fit_transform(actual)\n",
    "preds = mlb.transform(sub1['class'])\n",
    "print(\"f1-score:\",f1_score(actuals,preds,average='macro'))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"accuracy_score: \",accuracy_score(actuals,preds))\n",
    "from sklearn.metrics import jaccard_score\n",
    "print(\"jaccard_score: \",jaccard_score(actuals,preds,average='macro'))\n",
    "from sklearn.metrics import fbeta_score\n",
    "# pred_one =  np.asarray([np.ones(actuals.shape[1]) for _ in range(actuals.shape[0])])\n",
    "print(\"beta2:\",fbeta_score(actuals,preds,beta=2,average='samples'))\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "print(multilabel_confusion_matrix(actuals,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### moitoring\n",
    "Baseline model\n",
    "All ones:\n",
    "0.5612645781185107\n",
    "\n",
    "model valid:\n",
    "\n",
    "Thresh: 0.3\n",
    "beta2: 0.24638068678606243\n",
    "\n",
    "\n",
    "Thresh: 0.5\n",
    "beta2: 0.3153047127490837\n",
    "\n",
    "Thresh: 0.6\n",
    "beta2: 0.58 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = keras.models.load_model(\"./improvements1_1/weights-improvement-best_model.hdf5\",custom_objects={'fbeta':fbeta})\n",
    "predictions = {}\n",
    "files = glob.glob('./Test/*')\n",
    "c = 0\n",
    "for f in files:\n",
    "    fname = f.split('\\\\')[-1]\n",
    "    predictions[f\"image/{fname}\"] = make_label_predicts(f\"./Test/{fname}\",mod,thres=0.6)\n",
    "\n",
    "sub1 = pd.DataFrame({'image':predictions.keys(),'class':predictions.values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1.to_csv(\"../submissions/detectlabels.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extent Of damage\n",
    "* same steps as above.\n",
    "* used same model architecture and its pre-trained weights for detecting extent of damage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"newtrainlabels.csv\")\n",
    "df['image'] = df['image'].apply(lambda x: x.split('/')[-1])\n",
    "x = df.drop(\"extent_of_damage\",axis=1)\n",
    "y = df['extent_of_damage'].astype(str)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,stratify=y)\n",
    "train = pd.concat([x_train,y_train],axis=1)\n",
    "valid = pd.concat([x_test,y_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(dir_path,train,valid):\n",
    "\n",
    "    img_gen1 = ImageDataGenerator(rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        vertical_flip=True,\n",
    "        ) \n",
    "\n",
    "    train_ds = img_gen1.flow_from_dataframe(\n",
    "        dataframe=train,\n",
    "        directory=dir_path,\n",
    "        x_col=\"image\",\n",
    "        y_col=\"extent_of_damage\",\n",
    "        batch_size=12,\n",
    "        seed=0,\n",
    "        shuffle=True,\n",
    "        class_mode=\"categorical\",\n",
    "        classes=[\"0\",\"1\",\"2\",\"3\"],\n",
    "        target_size=(224,224)) # set as training data\n",
    "\n",
    "    img_gen2 = ImageDataGenerator(rescale=1./255,\n",
    "        ) \n",
    "    val_ds = img_gen2.flow_from_dataframe(\n",
    "        dataframe=valid,\n",
    "        directory=dir_path,\n",
    "        x_col=\"image\",\n",
    "        y_col=\"extent_of_damage\",\n",
    "        batch_size=12,\n",
    "        seed=0,\n",
    "        shuffle=True,\n",
    "        class_mode=\"categorical\",\n",
    "        classes=[\"0\",\"1\",\"2\",\"3\"],\n",
    "        target_size=(224,224),) # set as validation data\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 864 validated image filenames belonging to 4 classes.\n",
      "Found 217 validated image filenames belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\contelligenz\\venv\\lib\\site-packages\\keras\\preprocessing\\image.py:991: UserWarning: Found 2 invalid image filename(s) in x_col=\"image\". These filename(s) will be ignored.\n",
      "  n_invalid, x_col))\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds= load_dataset('./Train Copy',train.sample(train.shape[0]).reset_index(),valid.sample(valid.shape[0]).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.next()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0, '1': 1, '2': 2, '3': 3}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_model(in_shape=(224, 224, 3), out_shape=4):\n",
    "#   model = Sequential()\n",
    "#   model.add(Conv2D(32, (3, 3), activation='relu',  padding='same', input_shape=in_shape))\n",
    "#   model.add(Conv2D(32, (3, 3), activation='relu',  padding='same'))\n",
    "#   model.add(MaxPooling2D((2, 2)))\n",
    "#   model.add(Conv2D(64, (3, 3), activation='relu',  padding='same'))\n",
    "#   model.add(Dropout(0.2))\n",
    "#   model.add(Conv2D(64, (3, 3), activation='relu',  padding='same',kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "#   model.add(MaxPooling2D((2, 2)))\n",
    "#   model.add(Conv2D(128, (3, 3), activation='relu',  padding='same'))\n",
    "#   model.add(Dropout(0.2))\n",
    "#   model.add(Conv2D(128, (3, 3), activation='relu',  padding='same',kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "#   model.add(MaxPooling2D((2, 2)))\n",
    "#   model.add(Flatten())\n",
    "#   model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "#   model.add(Dense(out_shape, activation='softmax'))\n",
    "#   return model\n",
    "def model_detect(num_expressions=6):\n",
    "    model = VGG16(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224,224,3),\n",
    "    pooling='max',\n",
    "    classes=num_expressions,\n",
    ")\n",
    "    regularizer = tf.keras.regularizers.l2(0.1)\n",
    "    for layer in model.layers[:-5]:\n",
    "      layer.trainable= False\n",
    "    for layer in model.layers:\n",
    "      if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        print('Adding regularizer to layer {}'.format(layer.name))\n",
    "        layer.kernel_regularizer = regularizer\n",
    "    # model.get_layer('block5_conv1').trainable = True\n",
    "    # model.get_layer('block5_conv2').trainable = True\n",
    "    # model.get_layer('block5_conv3').trainable = True\n",
    "    # model.get_layer('block5_pool').trainable = True\n",
    "    flat1 = Flatten()(model.layers[-1].output)\n",
    "    layer1 = Dropout(0.4)(flat1)\n",
    "    layer2 = Dense(128, activation='relu', kernel_initializer='he_uniform')(layer1)\n",
    "    # layer3 = Dense(64, activation='relu', kernel_initializer='he_uniform')(layer2)\n",
    "    output = Dense(6, activation='sigmoid')(layer2)\n",
    "    # define new model\n",
    "    model = Model(inputs=model.inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.8868 - precision: 0.0000e+00\n",
      "Epoch 1: val_precision improved from -inf to 0.00000, saving model to ./improvements2_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 25s 315ms/step - loss: 161.8868 - precision: 0.0000e+00 - val_loss: 161.8698 - val_precision: 0.0000e+00\n",
      "Epoch 2/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.8032 - precision: 0.0000e+00\n",
      "Epoch 2: val_precision did not improve from 0.00000\n",
      "72/72 [==============================] - 22s 299ms/step - loss: 161.8032 - precision: 0.0000e+00 - val_loss: 161.7916 - val_precision: 0.0000e+00\n",
      "Epoch 3/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.7328 - precision: 0.0000e+00\n",
      "Epoch 3: val_precision did not improve from 0.00000\n",
      "72/72 [==============================] - 22s 298ms/step - loss: 161.7328 - precision: 0.0000e+00 - val_loss: 161.7299 - val_precision: 0.0000e+00\n",
      "Epoch 4/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.6738 - precision: 0.0000e+00\n",
      "Epoch 4: val_precision did not improve from 0.00000\n",
      "72/72 [==============================] - 21s 295ms/step - loss: 161.6738 - precision: 0.0000e+00 - val_loss: 161.6801 - val_precision: 0.0000e+00\n",
      "Epoch 5/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.6314 - precision: 0.8434\n",
      "Epoch 5: val_precision improved from 0.00000 to 0.56410, saving model to ./improvements2_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 61s 850ms/step - loss: 161.6314 - precision: 0.8434 - val_loss: 161.6380 - val_precision: 0.5641\n",
      "Epoch 6/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.5890 - precision: 0.5868\n",
      "Epoch 6: val_precision did not improve from 0.56410\n",
      "72/72 [==============================] - 104s 1s/step - loss: 161.5890 - precision: 0.5868 - val_loss: 161.6018 - val_precision: 0.5417\n",
      "Epoch 7/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.5583 - precision: 0.6080\n",
      "Epoch 7: val_precision did not improve from 0.56410\n",
      "72/72 [==============================] - 103s 1s/step - loss: 161.5583 - precision: 0.6080 - val_loss: 161.5696 - val_precision: 0.5490\n",
      "Epoch 8/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.5238 - precision: 0.6203\n",
      "Epoch 8: val_precision improved from 0.56410 to 0.66216, saving model to ./improvements2_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 71s 974ms/step - loss: 161.5238 - precision: 0.6203 - val_loss: 161.5423 - val_precision: 0.6622\n",
      "Epoch 9/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.4942 - precision: 0.7284\n",
      "Epoch 9: val_precision improved from 0.66216 to 0.74766, saving model to ./improvements2_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 161.4942 - precision: 0.7284 - val_loss: 161.5170 - val_precision: 0.7477\n",
      "Epoch 10/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.4771 - precision: 0.7294\n",
      "Epoch 10: val_precision improved from 0.74766 to 0.75676, saving model to ./improvements2_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 161.4771 - precision: 0.7294 - val_loss: 161.4962 - val_precision: 0.7568\n",
      "Epoch 11/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.4506 - precision: 0.7455\n",
      "Epoch 11: val_precision did not improve from 0.75676\n",
      "72/72 [==============================] - 13s 173ms/step - loss: 161.4506 - precision: 0.7455 - val_loss: 161.4699 - val_precision: 0.7500\n",
      "Epoch 12/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.4324 - precision: 0.7659\n",
      "Epoch 12: val_precision improved from 0.75676 to 0.77515, saving model to ./improvements2_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 161.4324 - precision: 0.7659 - val_loss: 161.4555 - val_precision: 0.7751\n",
      "Epoch 13/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.4201 - precision: 0.7758\n",
      "Epoch 13: val_precision improved from 0.77515 to 0.77907, saving model to ./improvements2_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 13s 182ms/step - loss: 161.4201 - precision: 0.7758 - val_loss: 161.4337 - val_precision: 0.7791\n",
      "Epoch 14/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.3988 - precision: 0.7710\n",
      "Epoch 14: val_precision improved from 0.77907 to 0.78824, saving model to ./improvements2_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 161.3988 - precision: 0.7710 - val_loss: 161.4174 - val_precision: 0.7882\n",
      "Epoch 15/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.3794 - precision: 0.7942\n",
      "Epoch 15: val_precision improved from 0.78824 to 0.79167, saving model to ./improvements2_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 13s 181ms/step - loss: 161.3794 - precision: 0.7942 - val_loss: 161.4065 - val_precision: 0.7917\n",
      "Epoch 16/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.3647 - precision: 0.7965\n",
      "Epoch 16: val_precision improved from 0.79167 to 0.80120, saving model to ./improvements2_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 161.3647 - precision: 0.7965 - val_loss: 161.3923 - val_precision: 0.8012\n",
      "Epoch 17/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.3531 - precision: 0.8146\n",
      "Epoch 17: val_precision improved from 0.80120 to 0.80723, saving model to ./improvements2_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 161.3531 - precision: 0.8146 - val_loss: 161.3777 - val_precision: 0.8072\n",
      "Epoch 18/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.3369 - precision: 0.8185\n",
      "Epoch 18: val_precision improved from 0.80723 to 0.81098, saving model to ./improvements2_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 13s 178ms/step - loss: 161.3369 - precision: 0.8185 - val_loss: 161.3670 - val_precision: 0.8110\n",
      "Epoch 19/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.3199 - precision: 0.8155\n",
      "Epoch 19: val_precision improved from 0.81098 to 0.81325, saving model to ./improvements2_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 161.3199 - precision: 0.8155 - val_loss: 161.3555 - val_precision: 0.8133\n",
      "Epoch 20/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.3208 - precision: 0.8307\n",
      "Epoch 20: val_precision improved from 0.81325 to 0.81548, saving model to ./improvements2_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 13s 183ms/step - loss: 161.3208 - precision: 0.8307 - val_loss: 161.3432 - val_precision: 0.8155\n",
      "Epoch 21/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.3071 - precision: 0.8283\n",
      "Epoch 21: val_precision improved from 0.81548 to 0.82635, saving model to ./improvements2_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 13s 185ms/step - loss: 161.3071 - precision: 0.8283 - val_loss: 161.3299 - val_precision: 0.8263\n",
      "Epoch 22/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.2941 - precision: 0.8353\n",
      "Epoch 22: val_precision did not improve from 0.82635\n",
      "72/72 [==============================] - 13s 176ms/step - loss: 161.2941 - precision: 0.8353 - val_loss: 161.3238 - val_precision: 0.8225\n",
      "Epoch 23/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.2879 - precision: 0.8300\n",
      "Epoch 23: val_precision improved from 0.82635 to 0.82941, saving model to ./improvements2_1\\weights-improvement-best_model.hdf5\n",
      "72/72 [==============================] - 13s 180ms/step - loss: 161.2879 - precision: 0.8300 - val_loss: 161.3079 - val_precision: 0.8294\n",
      "Epoch 24/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.2759 - precision: 0.8345\n",
      "Epoch 24: val_precision did not improve from 0.82941\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 161.2759 - precision: 0.8345 - val_loss: 161.3058 - val_precision: 0.8235\n",
      "Epoch 25/25\n",
      "72/72 [==============================] - ETA: 0s - loss: 161.2662 - precision: 0.8343\n",
      "Epoch 25: val_precision did not improve from 0.82941\n",
      "72/72 [==============================] - 13s 177ms/step - loss: 161.2662 - precision: 0.8343 - val_loss: 161.2971 - val_precision: 0.8235\n"
     ]
    }
   ],
   "source": [
    "model1 = keras.models.load_model(\"./improvements1_1/weights-improvement-best_model.hdf5\",custom_objects={'fbeta':fbeta})\n",
    "# model2 = model_detect()\n",
    "model2 =Sequential()\n",
    "for layer in model1.layers:\n",
    "   model2.add(layer)\n",
    "# Freeze the layers \n",
    "for layer in model2.layers:\n",
    "    layer.trainable = False\n",
    "model2.add(Dense(4, activation='softmax'))\n",
    "model2.compile(optimizer=Adam(0.001), loss=CategoricalCrossentropy(from_logits=False),metrics=keras.metrics.Precision(name='precision'))\n",
    "filepath=\"./improvements2_1/weights-improvement-best_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_precision', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "hist = model2.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch = train_ds.samples // 12,\n",
    "    validation_data = val_ds, \n",
    "    validation_steps = val_ds.samples // 12,\n",
    "    epochs = 25,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import load_img, img_to_array\n",
    "\n",
    "def make_label_predicts(filename,model,thres=0.5):\n",
    "    labs = {0:'0',\n",
    " 1:'1',  \n",
    " 2:'2',\n",
    " 3:'3',\n",
    " 4:'4'}\n",
    "    img = cv2.imread(f\"{filename}\")\n",
    "    if img is not None:\n",
    "        img = cv2.resize(img,(224, 224),interpolation=cv2.INTER_AREA)\n",
    "        img = np.array(img) / 225.0\n",
    "        img = img.reshape(-1,224,224,3)\n",
    "        pred = np.squeeze(model.predict(img,verbose=0))\n",
    "        return labs[np.argmax(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.load_model(\"./improvements2_1/weights-improvement-best_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score: 0.5973886229594672\n",
      "accuracy_score:  0.8002496878901373\n",
      "jaccard_score:  0.5132688939208994\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "files = glob.glob('./Train/*')\n",
    "c = 0\n",
    "for f in files:\n",
    "    fname = f.split('\\\\')[-1]\n",
    "    predictions[f\"image/{fname}\"] = make_label_predicts(f\"./Train/{fname}\",model2,thres=0.6)\n",
    "\n",
    "sub1 = pd.DataFrame({'image':predictions.keys(),'extent_of_damage':predictions.values()})\n",
    "labeldf = pd.read_csv('./trainlabels.csv')\n",
    "hash_map  = dict(labeldf[['image','extent_of_damage']].values)\n",
    "actual = sub1['image'].apply(lambda x:str(hash_map[x]))\n",
    "\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# actuals = mlb.fit_transform(actual)\n",
    "# preds = mlb.transform(sub1['class'])\n",
    "print(\"f1-score:\",f1_score(actual,sub1['extent_of_damage'],average='macro'))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"accuracy_score: \",accuracy_score(actual,sub1['extent_of_damage']))\n",
    "from sklearn.metrics import jaccard_score\n",
    "print(\"jaccard_score: \",jaccard_score(actual,sub1['extent_of_damage'],average='macro'))\n",
    "from sklearn.metrics import fbeta_score\n",
    "# pred_one =  np.asarray([np.ones(actuals.shape[1]) for _ in range(actuals.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1-score: 0.6276695790006568\n",
    "accuracy_score:  0.7116104868913857\n",
    "jaccard_score:  0.4674106760167673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = keras.models.load_model(\"./improvements2_2/weights-improvement-best_model.hdf5\",custom_objects={'fbeta':fbeta})\n",
    "predictions = {}\n",
    "files = glob.glob('./Test/*')\n",
    "c = 0\n",
    "for f in files:\n",
    "    fname = f.split('\\\\')[-1]\n",
    "    predictions[f\"image/{fname}\"] = make_label_predicts(f\"./Test/{fname}\",model2,thres=0.6)\n",
    "\n",
    "sub1 = pd.DataFrame({'image':predictions.keys(),'extent_of_damage':predictions.values()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1.to_csv('../submissions/extentlabels.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "damagelabels = pd.read_csv(\"../submissions/detectlabels.csv\")\n",
    "extentlabels = pd.read_csv(\"../submissions/extentlabels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image/1000.jpeg</td>\n",
       "      <td>['dent']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image/1001.jpeg</td>\n",
       "      <td>['scratch']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image/1002.jpeg</td>\n",
       "      <td>['glass_shatter']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image/1003.jpeg</td>\n",
       "      <td>['unknown']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image/1004.jpeg</td>\n",
       "      <td>['tail_lamp']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image              class\n",
       "0  image/1000.jpeg           ['dent']\n",
       "1  image/1001.jpeg        ['scratch']\n",
       "2  image/1002.jpeg  ['glass_shatter']\n",
       "3  image/1003.jpeg        ['unknown']\n",
       "4  image/1004.jpeg      ['tail_lamp']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damagelabels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>extent_of_damage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image/1000.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image/1001.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image/1002.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image/1003.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image/1004.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image  extent_of_damage\n",
       "0  image/1000.jpeg                 1\n",
       "1  image/1001.jpeg                 1\n",
       "2  image/1002.jpeg                 1\n",
       "3  image/1003.jpeg                 0\n",
       "4  image/1004.jpeg                 1"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extentlabels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsub = pd.merge(damagelabels,extentlabels,on='image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image               0\n",
       "class               0\n",
       "extent_of_damage    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalsub.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsub['dent'] = finalsub['class'].apply(lambda x: 1 if 'dent' in x else 0)\n",
    "finalsub['glass_shatter'] = finalsub['class'].apply(lambda x: 1 if 'glass_shatter' in x else 0)\n",
    "finalsub['head_lamp'] = finalsub['class'].apply(lambda x: 1 if 'head_lamp' in x else 0)\n",
    "finalsub['scratch'] = finalsub['class'].apply(lambda x: 1 if 'scratch' in x else 0)\n",
    "finalsub['tail_lamp'] = finalsub['class'].apply(lambda x: 1 if 'tail_lamp' in x else 0)\n",
    "finalsub['unknown'] = finalsub['class'].apply(lambda x: 1 if 'unknown' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "      <th>extent_of_damage</th>\n",
       "      <th>dent</th>\n",
       "      <th>glass_shatter</th>\n",
       "      <th>head_lamp</th>\n",
       "      <th>scratch</th>\n",
       "      <th>tail_lamp</th>\n",
       "      <th>unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image/1000.jpeg</td>\n",
       "      <td>['dent']</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image/1001.jpeg</td>\n",
       "      <td>['scratch']</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image/1002.jpeg</td>\n",
       "      <td>['glass_shatter']</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image/1003.jpeg</td>\n",
       "      <td>['unknown']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image/1004.jpeg</td>\n",
       "      <td>['tail_lamp']</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image              class  extent_of_damage  dent  glass_shatter  \\\n",
       "0  image/1000.jpeg           ['dent']                 1     1              0   \n",
       "1  image/1001.jpeg        ['scratch']                 1     0              0   \n",
       "2  image/1002.jpeg  ['glass_shatter']                 1     0              1   \n",
       "3  image/1003.jpeg        ['unknown']                 0     0              0   \n",
       "4  image/1004.jpeg      ['tail_lamp']                 1     0              0   \n",
       "\n",
       "   head_lamp  scratch  tail_lamp  unknown  \n",
       "0          0        0          0        0  \n",
       "1          0        1          0        0  \n",
       "2          0        0          0        0  \n",
       "3          0        0          0        1  \n",
       "4          0        0          1        0  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalsub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(801, 1100)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext = finalsub['extent_of_damage']\n",
    "sub = finalsub.drop(['class','extent_of_damage'],axis=1)\n",
    "sub['extent_of_damage'] = ext\n",
    "f = sub['image'].apply(lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "f.min(), f.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['image_num'] = sub['image'].apply(lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "sub = sub.sort_values('image_num',ascending=True)\n",
    "sub = sub.drop(['image_num'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>dent</th>\n",
       "      <th>glass_shatter</th>\n",
       "      <th>head_lamp</th>\n",
       "      <th>scratch</th>\n",
       "      <th>tail_lamp</th>\n",
       "      <th>unknown</th>\n",
       "      <th>extent_of_damage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>image/801.jpeg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>image/802.jpeg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>image/803.jpeg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>image/804.jpeg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>image/805.jpeg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              image  dent  glass_shatter  head_lamp  scratch  tail_lamp  \\\n",
       "101  image/801.jpeg     0              0          0        0          1   \n",
       "102  image/802.jpeg     1              0          1        1          0   \n",
       "103  image/803.jpeg     0              0          0        0          0   \n",
       "104  image/804.jpeg     1              0          0        0          0   \n",
       "105  image/805.jpeg     1              0          0        1          0   \n",
       "\n",
       "     unknown  extent_of_damage  \n",
       "101        0                 1  \n",
       "102        0                 1  \n",
       "103        1                 0  \n",
       "104        0                 1  \n",
       "105        0                 1  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_excel(\"../submissions/prediction_atufa.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function\n",
    "1. Load imports\n",
    "2. Run cells of this section\n",
    "3. run the function predict_image(<image_path>,<model_detect_path>,<model_extent_path>)\n",
    "Note: \n",
    "* detectweights-model is model_detect_path\n",
    "* extentweights-model is model_extent model_extent_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label_predicts(filename,model,thres=0.5):\n",
    "    labs = {0:'0',\n",
    " 1:'1',  \n",
    " 2:'2',\n",
    " 3:'3',\n",
    " 4:'4'}\n",
    "    img = cv2.imread(f\"{filename}\")\n",
    "    if img is not None:\n",
    "        img = cv2.resize(img,(224, 224),interpolation=cv2.INTER_AREA)\n",
    "        img = np.array(img) / 225.0\n",
    "        img = img.reshape(-1,224,224,3)\n",
    "        pred = np.squeeze(model.predict(img,verbose=0))\n",
    "        return labs[np.argmax(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import load_img, img_to_array\n",
    "\n",
    "def make_label_predicts(filename,model,thres=0.5):\n",
    "    labs = {0:'unknown',\n",
    " 1:'dent',  \n",
    " 2:'scratch',\n",
    " 3:'glass_shatter',\n",
    " 4:'head_lamp',\n",
    " 5:'tail_lamp'}\n",
    "    img = cv2.imread(f\"{filename}\")\n",
    "    if img is not None:\n",
    "        img = cv2.resize(img,(224, 224),interpolation=cv2.INTER_AREA)\n",
    "        img = np.array(img) / 225.0\n",
    "        img = img.reshape(-1,224,224,3)\n",
    "        pred = np.squeeze(model.predict(img,verbose=0))\n",
    "        labels = []\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i]>=thres:\n",
    "                labels.append(labs[i])\n",
    "        if len(labels)==0:\n",
    "            labels.append('unknown')\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend\n",
    " \n",
    "# calculate fbeta score for multi-class/label classification\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "\t# clip predictions\n",
    "\ty_pred = backend.clip(y_pred, 0, 1)\n",
    "\t# calculate elements\n",
    "\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "\t# calculate precision\n",
    "\tp = tp / (tp + fp + backend.epsilon())\n",
    "\t# calculate recall\n",
    "\tr = tp / (tp + fn + backend.epsilon())\n",
    "\t# calculate fbeta, averaged across each class\n",
    "\tbb = beta ** 2\n",
    "\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
    "\treturn fbeta_score\n",
    "\n",
    "def damage_detect(image_path,model_damage,thresh=0.6):\n",
    "        labs = {0:'unknown',\n",
    "                1:'dent',  \n",
    "                2:'scratch',\n",
    "                3:'glass_shatter',\n",
    "                4:'head_lamp',\n",
    "                5:'tail_lamp'}\n",
    "        img = cv2.imread(f\"{image_path}\")\n",
    "        labels=[]\n",
    "        if img is not None:\n",
    "                img = cv2.resize(img,(224, 224),interpolation=cv2.INTER_AREA)\n",
    "                img = np.array(img) / 225.0\n",
    "                img = img.reshape(-1,224,224,3)\n",
    "                pred = np.squeeze(model_damage.predict(img,verbose=0))\n",
    "                labels = []\n",
    "                for i in range(len(pred)):\n",
    "                        if pred[i]>=thresh:\n",
    "                                labels.append(labs[i])\n",
    "                if len(labels)==0:\n",
    "                        labels.append('unknown')\n",
    "        else:\n",
    "                return \"Image cannot be read!\"\n",
    "        return labels\n",
    "\n",
    "def extent_detect(image_path,model_extent):\n",
    "        labs = {0:'0',\n",
    "                1:'1',  \n",
    "                2:'2',\n",
    "                3:'3',\n",
    "                4:'4'}\n",
    "        img = cv2.imread(f\"{image_path}\")\n",
    "        if img is not None:\n",
    "                img = cv2.resize(img,(224, 224),interpolation=cv2.INTER_AREA)\n",
    "                img = np.array(img) / 225.0\n",
    "                img = img.reshape(-1,224,224,3)\n",
    "                pred = np.squeeze(model_extent.predict(img,verbose=0))\n",
    "                return labs[np.argmax(pred)]\n",
    "\n",
    "def predict_image(image_path,model_detect_path,model_extent_path):\n",
    "        model_detect = keras.models.load_model(model_detect_path,custom_objects={'fbeta':fbeta})\n",
    "        labels = damage_detect(image_path,model_detect)\n",
    "        model_extent = keras.models.load_model(model_extent_path,custom_objects={'fbeta':fbeta})\n",
    "        extent = extent_detect(image_path,model_extent)\n",
    "        dent = 1 if 'dent' in labels else 0\n",
    "        glass_shatter = 1 if 'glass_shatter' in labels else 0\n",
    "        head_lamp = 1 if 'head_lamp' in labels else 0\n",
    "        scratch = 1 if 'scratch' in labels else 0\n",
    "        tail_lamp = 1 if 'tail_lamp' in labels else 0\n",
    "        unknown = 1 if 'unknown' in labels else 0\n",
    "        return {'image':image_path,'dent':dent,'glass_shatter':glass_shatter,'head_lamp':head_lamp,'scratch':scratch,'tail_lamp':tail_lamp,'unknown':unknown,'extent_of_damage':extent}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': './Train/0.jpeg',\n",
       " 'dent': 1,\n",
       " 'glass_shatter': 0,\n",
       " 'head_lamp': 1,\n",
       " 'scratch': 0,\n",
       " 'tail_lamp': 0,\n",
       " 'unknown': 0,\n",
       " 'extent_of_damage': '1'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image('./Train/0.jpeg','../submissions/detectweights-model.hdf5','../submissions/extentweights-model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8e211089d2b60f9938a6d5435dc96b9dea5d85f2b5b4958aabb858d24a9de17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
